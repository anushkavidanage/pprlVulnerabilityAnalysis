# locality_sensitive_hashing.py - Module that implements several vulnerability tests for 
# plaintext and encoded data
#
# July 2020

# Anushka Vidanage, Peter Christen, Thilina Ranbaduge, and Rainer Schnell
#
# Contact: anushka.vidanage@anu.edu.au
#
# Research School of Computer Science, The Australian National University,
# Canberra, ACT, 2601
# -----------------------------------------------------------------------------
#
# Copyright 2020 Australian National University and others.
# All Rights reserved.
#
# -----------------------------------------------------------------------------
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
# =============================================================================

import binascii
import random
import numpy
import bitarray
import time


class MinHashLSH():
  """A class that implements a min-hashing locality sensitive hashing (LSH)
     approach to be used for blocking the plain-text q-grams sets in order to
     prevent a full-pair-wise comparison of all q-gram set pairs.
  """

  def __init__(self, lsh_band_size, lsh_num_band, random_seed=None):
    """Initialise the parameters for min-hashing LSH including generating
       random values for hash functions.

       Input arguments:
         - lsh_band_size  The length of the min-hash bands.
         - lsh_num_band   The number of LSH bands.
         - random_seed    If not None then initalise the random number
                          generator with this seed value.

       Output:
         - This method does not return anything.

       LSH min-hashing follows the code provided here:
        https://github.com/chrisjmccormick/MinHash/blob/master/ \
              runMinHashExample.py

       The probability for a pair of sets with Jaccard sim 0 < s <= 1 to be
       included as a candidate pair is (with b = lsh_num_band and
       r = lsh_band_size, i.e. the number of rows/hash functions per band) is
       (Leskovek et al., 2014, page 89):

         p_cand = 1- (1 - s^r)^b

       Approximation of the 'threshold' of the S-curve (Leskovek et al., 2014,
       page 90) is: t = (1/k)^(1/r).
    """

    if (random_seed != None):
      random.seed(random_seed)

    # Calculate error probabilities for given parameter values
    #
    assert lsh_num_band >  1, lsh_num_band
    assert lsh_band_size > 1, lsh_band_size

    self.lsh_num_band =   lsh_num_band
    self.lsh_band_size =  lsh_band_size
    self.num_hash_funct = lsh_band_size*lsh_num_band  # Total number needed

    b = float(lsh_num_band)
    r = float(lsh_band_size)
    t = (1.0/b)**(1.0/r)

    s_p_cand_list = []
    for i in range(1,10):
      s = 0.1*i
      p_cand = 1.0-(1.0-s**r)**b
      assert 0.0 <= p_cand <= 1.0
      s_p_cand_list.append((s, p_cand))

    print 'Initialise LSH blocking using Min-Hash'
    print '  Number of hash functions: %d' % (self.num_hash_funct)
    print '  Number of bands:          %d' % (lsh_num_band)
    print '  Size of bands:            %d' % (lsh_band_size)
    print '  Threshold of s-curve:     %.3f' % (t)
    print '  Probabilities for candidate pairs:'
    print '   Jacc_sim | prob(cand)'
    for (s,p_cand) in s_p_cand_list:
      print '     %.2f   |   %.5f' % (s, p_cand)
    print

    max_hash_val = 2**31-1  # Maximum possible value a CRC hash could have

    # We need the next largest prime number above 'maxShingleID'.
    # From here:
    # http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php
    #
    self.next_prime = 4294967311

    # Random hash function will take the form of: h(x) = (a*x + b) % c
    # where 'x' is the input value, 'a' and 'b' are random coefficients, and
    # 'c' is a prime number just greater than max_hash_val
    #
    # Generate 'num_hash_funct' coefficients
    #
    coeff_a_set = set()
    coeff_b_set = set()

    while (len(coeff_a_set) < self.num_hash_funct): 
      coeff_a_set.add(random.randint(0, max_hash_val))
    while (len(coeff_b_set) < self.num_hash_funct): 
      coeff_b_set.add(random.randint(0, max_hash_val))
    self.coeff_a_list = sorted(coeff_a_set)
    self.coeff_b_list = sorted(coeff_b_set)
    assert self.coeff_a_list != self.coeff_b_list

  # ---------------------------------------------------------------------------

  def hash_q_gram_set(self, q_gram_set):
    """Min-hash the given set of q-grams and return a list of hash signatures
       depending upon the Min-hash parameters set during the class
       initialisation.

       Input arguments:
         - q_gram_set  The q-gram set to be hashed.

       Output:
         -  band_hash_sig_list  A list with the min-hash signatures for the
                                input q-gram set.
    """

    next_prime =    self.next_prime
    coeff_a_list =  self.coeff_a_list
    coeff_b_list =  self.coeff_b_list
    lsh_band_size = self.lsh_band_size
    lsh_num_band =  self.lsh_num_band

    crc_hash_set = set()

    for q_gram in q_gram_set:  # Hash the q-grams into 32-bit integers
      crc_hash_set.add(binascii.crc32(q_gram) & 0xffffffff)

    assert len(q_gram_set) == len(crc_hash_set)  # Check no collision

    # Now generate all the min-hash values for this q-gram set
    #
    min_hash_sig_list = []

    for h in range(self.num_hash_funct):
 
      # For each CRC hash value (q-gram) in the q-gram set calculate its Min-
      # hash value for all 'num_hash_funct' functions
      #
      min_hash_val = next_prime + 1  # Initialise to value outside range

      for crc_hash_val in crc_hash_set:
        hash_val = (coeff_a_list[h]*crc_hash_val + coeff_b_list[h]) % \
                   next_prime
        min_hash_val = min(min_hash_val, hash_val)

      min_hash_sig_list.append(min_hash_val)

    # Now split hash values into bands and generate the list of
    # 'lsh_num_band' hash values used for blocking
    #
    band_hash_sig_list = []

    start_ind = 0
    end_ind =   lsh_band_size
    for band_num in range(lsh_num_band):
      band_hash_sig = min_hash_sig_list[start_ind:end_ind]
      assert len(band_hash_sig) == lsh_band_size
      start_ind = end_ind
      end_ind +=  lsh_band_size
      band_hash_sig_list.append(band_hash_sig)

    return band_hash_sig_list

# =============================================================================

class CosineLSH():
  """A class that implements a random vector based Cosine similarity locality
     sensitive hashing (LSH) approach.

     As described in the paper:
       A. Andoni and P. Indyk: Near-optimal hashing algorithms for approximate
       nearest neighbor in high dimensions. In FOCS. IEEE, 2006.

     Based on code from: https://www.bogotobogo.com/Algorithms/ \
         Locality_Sensitive_Hashing_LSH_using_Cosine_Distance_Similarity.php
  """
  
  def __init__(self, vec_len, num_hash_sign_len, random_seed=None):
    """Initialise the LSH approach by generating a certain number of random
       vectors.

       Input arguments:
         - vec_len            The dimensionality of the feature vectors to be
                              hashed.
         - num_hash_sign_len  The length of the bit arrays to be generated
                              (i.e. the number of random hyper-planes to be
                              generated).
         - random_seed        If not None then initalise the random number
                              generator with this seed value

       Output:
         - This method does not return anything.
    """

    if (random_seed != None):
      numpy.random.seed(random_seed)

    self.vec_len =           vec_len
    self.num_hash_sign_len = num_hash_sign_len
    
    # Generate the required number of random hyper-planes (num_hash_sign_len),
    # each a vector of length 'vec_len'
    #
    self.ref_planes = numpy.random.randn(num_hash_sign_len, vec_len)

    print 'Initialised CosineLSH by generating %d hyper-planes each of ' % \
          (num_hash_sign_len) + 'dimensionality %d' % (vec_len)
    print

  # ---------------------------------------------------------------------------

  def gen_sim_hash(self, feature_dict):
    """Encode each of the feature vectors in the given dictionary using the
       random hyper-planes resulting a bit-array for each feature vector.

       Input arguments:
         - feature_dict  The feature dictionary with keys being node values
                         and values being lists of numerical feature values.

       Output:
         - sim_hash_dict  A dictionary with the calculated hash bit arrays,
                          with keys being node values and values bit arrays.
    """

    num_hash_sign_len = self.num_hash_sign_len
    vec_len =           self.vec_len

    sim_hash_dict = {}

    ref_planes = self.ref_planes

    # To allow calculation of statistics, keep a counter of how many bits are
    # set to 1 in each bit position
    #
    bit_pos_1_count_list = [0]*num_hash_sign_len

    # Also get the Hamming weight (number of 1-bits) distribution over all
    # bit arrays
    #
    hw_list = []

    for (val, feat_array) in feature_dict.iteritems():
      assert len(feat_array) == vec_len

      hash_bit_array = bitarray.bitarray(num_hash_sign_len)
      hash_bit_array.setall(0)

      # Generate the hash bit signature based on the given random hyper-planes
      #
      for (bit_pos, rand_vec) in enumerate(ref_planes):
        if (numpy.dot(feat_array, rand_vec) >= 0):
          hash_bit_array[bit_pos] = 1
          bit_pos_1_count_list[bit_pos] += 1

      sim_hash_dict[val] = hash_bit_array
      hw_list.append(int(hash_bit_array.count(1)))

    # Check the number of 0/1 bits for each position in the bit arrays and
    # print statistics of these distributions
    #
    print 'Statistics for sim hash encoding of %d feature vectors:' % \
          (len(sim_hash_dict))
    print '  Minimum, average (std-dev), median and maximum number of 1-' + \
          'bits per position:  %d / %.3f (%.3f) / %d / %d (from %d in total)' \
          % (min(bit_pos_1_count_list), numpy.mean(bit_pos_1_count_list),
             numpy.std(bit_pos_1_count_list),
             numpy.median(bit_pos_1_count_list), max(bit_pos_1_count_list),
             len(sim_hash_dict))
    print '  Minimum, average (std-dev), median and maximum number of 1-' + \
          'bits per bit array: %d / %.3f (%.3f) / %d / %d (from %d in total)' \
          % (min(hw_list), numpy.mean(hw_list), numpy.std(hw_list),
             numpy.median(hw_list), max(hw_list),num_hash_sign_len)
    print

    return sim_hash_dict

  # ---------------------------------------------------------------------------

  def all_in_one_blocking(self, val_sim_hash_dict):
    """Generate two blocking dictionaries, one per input similarity hash
       dictionary, where all records from an input dictionary are put into
       one block. Returns these two blocking dictionaries.

       Input arguments:
         - plain_sim_hash_dict   A dictionary with keys being node values and
                                 values being hash bit arrays.
         - encode_sim_hash_dict  A dictionary with keys being node values and
                                 values being hash bit arrays.

       Output:
         - plain_sim_block_dict    A dictionary with one key being 'all' and
                                   values being being node key values from the
                                   plain-text similarity hash dictionary.
         - encode_sim_block_dict   A dictionary with one key being 'all' and
                                   values being being node key values from the
                                   encoded similarity hash dictionary.
    """

    val_sim_block_dict =  {'all':set(val_sim_hash_dict.keys())}

    print 'Number of blocks for the all-in-one index: 1 / 1'
    print '  Block size: %d' %len(plain_sim_block_dict['all'])
    print

    return val_sim_block_dict

  # ---------------------------------------------------------------------------

  def hlsh_blocking(self, val_sim_hash_dict, hlsh_sample_size, 
                    hlsh_num_sample, bit_array_len, random_seed=None):
    """Generate blocks from the similarity hash dictionaries using Hamming
       locality sensitive hashing (HLSH) on the bit arrays representing nodes
       to limit comparison only of those nodes with a certain Hamming
       similarity. Returns these two blocking dictionaries.

       Input arguments:
         - plain_sim_hash_dict   A dictionary with keys being node values and
                                 values being hash bit arrays.
         - encode_sim_hash_dict  A dictionary with keys being node values and
                                 values being hash bit arrays.
         - hlsh_sample_size      The number of bits to sample for each HLSH
                                 block.
         - hlsh_num_sample       The number of times to sample from each bit
                                 array.
         - bit_array_len         The total length of the bit arrays in the
                                 similarity hash dictionaries.
         - random_seed           If not None then initialise the random number
                                 generator with this seed value

       Output:
         - plain_sim_block_dict    A dictionary with keys being HLSH values
                                   and values being being node key values from
                                   the plain-text similarity hash dictionary.
         - encode_sim_block_dict   A dictionary with keys being HLSH values
                                   and values being being node key values from
                                   the encoded similarity hash dictionary.
    """

    start_time = time.time()

    plain_sim_block_dict =  {}  # The dictionaries to be returned
    encode_sim_block_dict = {}
    
    val_sim_block_dict =  {}  # The dictionary to be returned

    if (random_seed != None):
      numpy.random.seed(random_seed)

    # First generate the required list of bit position arrays to be used for
    # sampling ('hlsh_num_sample' arrays each of length 'hlsh_sample_size')
    #
    bit_sample_list = []

    for sample_num in xrange(hlsh_num_sample):
      bit_sample_list.append(random.sample(xrange(bit_array_len),
                                           hlsh_sample_size))
      if (len(bit_sample_list) > 1):
        assert bit_sample_list[-1] != bit_sample_list[-2]  # Check uniqueness

    # Loop over all similarity hash dictionary keys and bit arrays
    #
    for (key_val, dict_bit_array) in val_sim_hash_dict.iteritems():

      # Loop over all HSLH bit position sample lists
      #
      for (sample_num, bit_pos_list) in enumerate(bit_sample_list):
        sample_bit_array = bitarray.bitarray(hlsh_sample_size)
        sample_num_str = str(sample_num)+'-'

        for (i, bit_pos) in enumerate(bit_pos_list):
          sample_bit_array[i] = dict_bit_array[bit_pos]

        # Generate the HLSH block (dictionary) key
        #
        hlsh_dict_key = sample_num_str+sample_bit_array.to01()

        # Add the current key value into the corresponding HLSH block
        #
        hlsh_block_key_set = val_sim_block_dict.get(hlsh_dict_key, set())
        hlsh_block_key_set.add(key_val)
        val_sim_block_dict[hlsh_dict_key] = hlsh_block_key_set
    
    # Print summary statistics about the generated LSH blocks
    #
    print 'Number of blocks for the HLSH index: %d' % \
          len(val_sim_block_dict) + \
          '  (with sample size: %d, and number of samples: %d' % \
          (hlsh_sample_size, hlsh_num_sample)
    hlsh_block_size_list = []
    for hlsh_block_key_set in val_sim_block_dict.itervalues():
      hlsh_block_size_list.append(len(hlsh_block_key_set))
    print '  Minimum, average, median and maximum block sizes: ' + \
          '%d / %.2f / %d / %d' % (min(hlsh_block_size_list),
                               numpy.mean(hlsh_block_size_list),
                               numpy.median(hlsh_block_size_list),
                               max(hlsh_block_size_list))

    return val_sim_block_dict
  
  # ---------------------------------------------------------------------------

  def calc_cos_sim_all_pairs(self, plain_sim_hash_dict,
                             encode_sim_hash_dict, plain_sim_block_dict,
                             encode_sim_block_dict, hash_min_sim):
    """Calculate the Cosine similarity of all node pairs in the same blocks
       between the two graphs based on the LSH values in the given two
       block dictionaries. Only keep those pairs with a similarity above the
       given threshold.

       Input arguments:
         - plain_sim_hash_dict     A dictionary with keys being node key
                                   values and values being hash bit arrays.
         - encode_sim_hash_dict    A dictionary with keys being node key
                                   values and values being hash bit arrays.
         - plain_sim_block_dict    A dictionary with keys being block key
                                   values and values being being node key
                                   values from the plain-text similarity hash
                                   dictionary.
         - encode_sim_block_dict   A dictionary with keys being block key
                                   values and values being being node key
                                   values from the encoded similarity hash
                                   dictionary.
          - hash_min_sim           The minimum similarity required for a pair
                                   of identifiers to be stored in the
                                   dictionary to be returned.
       Output:
         - hash_sim_dict  A dictionary with node key value pairs as keys and
                          their hash similarities as values.
    """

    start_time = time.time()
    
    hash_sim_dict = {}

    num_pairs_compared = 0
    comp_pairs_dict = {}  # For each key from the plain-text dictionary a set
                          # of keys from the encoded dictionary

    # Loop over all blocks in both block dictionaries
    #
    for (plain_block_key, plain_key_set) in plain_sim_block_dict.iteritems():
      encode_key_set = encode_sim_block_dict.get(plain_block_key, set())

      for key_val1 in plain_key_set:
        bit_array1 =     plain_sim_hash_dict[key_val1]
        bit_array1_len = len(bit_array1)

        encode_comp_key_set = comp_pairs_dict.get(key_val1, set())

        for key_val2 in encode_key_set:
          if (key_val2 in encode_comp_key_set):  # An already compared pair
            continue

          encode_comp_key_set.add(key_val2)  # Mark as being compared

          bit_array2 = encode_sim_hash_dict[key_val2]

          num_pairs_compared += 1

          # Get number of differing 1-bits
          #
          ba_xor_hw = (bit_array1 ^ bit_array2).count(1)

          # Calculate the angle difference using LSH based on Cosine distance
          #
          cos_hash_sim = 1.0 - float(ba_xor_hw) / bit_array1_len

          if (cos_hash_sim >= hash_min_sim):
            hash_sim_dict[(key_val1, key_val2)] = cos_hash_sim

        comp_pairs_dict[key_val1] = encode_comp_key_set

    print 'Compared %d record pairs based on their Cosine LSH hash values' % \
          (num_pairs_compared) + ' (using all-pair comparison)'
    print '  %d of these pairs had a similarity of at least %.2f' % \
          (len(hash_sim_dict), hash_min_sim)

    print '  Time used: %.3f sec' % (time.time() - start_time)
    print

    comp_pairs_dict = {}  # Not needed anymore

    return hash_sim_dict

  # ---------------------------------------------------------------------------